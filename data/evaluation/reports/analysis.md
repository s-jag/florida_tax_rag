# Florida Tax RAG Evaluation Analysis

**Analysis Date:** 2026-01-06
**Dataset Version:** 1.0.0
**Total Questions in Dataset:** 20 (5 easy, 10 medium, 5 hard)

---

## Executive Summary

The evaluation pipeline successfully ran against the Florida Tax RAG system. Initial results show **strong citation precision** (75-100% on successful responses) but reveal several infrastructure issues that need resolution before production deployment.

### Key Metrics (Baseline)

| Metric | Run 1 (3 Qs) | Run 2 (5 Qs) | Notes |
|--------|--------------|--------------|-------|
| Citation Precision | 76.7% | 90.0% | Strong - generated citations are accurate |
| Citation Recall | 50.0% | 40.0% | Moderate - missing some expected citations |
| Citation F1 | 60.5% | 55.4% | Balanced measure |
| Answer Contains | 38.9% | 20.0% | Expected phrases found in answers |
| Pass Rate | 0.0% | 0.0% | LLM judge not scoring (see issues) |
| Avg Latency | 71,550ms | 35,320ms | High - needs optimization |
| Hallucinations | 0 | 0 | No fabricated citations detected |

---

## Detailed Findings

### 1. Answer Quality (When System Works)

The RAG system produces **high-quality, comprehensive answers** when it successfully retrieves context. Example from eval_001 ("What is the Florida state sales tax rate?"):

**Generated Answer Structure:**
- Direct answer: "The Florida state sales tax rate is 6%"
- Legal basis with statute citations
- Regulatory guidance references
- Exceptions (4.35% electrical power, 4% amusement, 3% mobile homes)
- Caveats about county surtaxes

**Strengths:**
- Answers are well-structured with clear headings
- Citations are inline with source references
- Exceptions and edge cases are addressed
- No hallucinated statutes or incorrect rates

### 2. Citation Analysis

#### Precision (Generated Citations That Are Correct)

| Question | Precision | Analysis |
|----------|-----------|----------|
| eval_001 | 75-80% | Cited extra sections (212.05(1)(b), (c), (e), (h), (n)) beyond expected |
| eval_002 | 75% | Comprehensive food exemption citations |
| eval_003 | 50% | Many dealer definition subsections cited |

**Finding:** The system cites MORE sources than expected, which reduces precision but demonstrates thoroughness. This is acceptable behavior for a legal research tool.

#### Recall (Expected Citations Found)

| Question | Recall | Missing |
|----------|--------|---------|
| eval_001 | 100% | None - found 212.05 |
| eval_002 | 100% | None - found 212.08 |
| eval_003 | 50% | Some expected citations missed due to rate limiting |

**Finding:** When answers complete successfully, recall is strong. Rate limiting caused incomplete responses.

### 3. Performance by Category

| Category | Questions | Precision | Recall | Issues |
|----------|-----------|-----------|--------|--------|
| sales_tax | 2 | 65-88% | 50-100% | Variable based on question complexity |
| exemptions | 1 | 75-100% | 0-100% | Rate limiting affected some runs |
| corporate_tax | 1 | 100% | 0% | Rate limited - no answer generated |
| procedures | 1 | 100% | 0% | Rate limited - no answer generated |

### 4. Latency Distribution

| Question | Latency | Notes |
|----------|---------|-------|
| eval_001 | 35-101s | Complex multi-retrieval query |
| eval_002 | 11-101s | Food exemption (many exceptions) |
| eval_003 | 13-103s | Rate limited or comprehensive |
| eval_004 | 14s | Rate limited (no answer) |
| eval_005 | 13s | Rate limited (no answer) |

**Finding:** Successful complex answers take 35-100+ seconds. This includes:
- Weaviate retrieval (~2-5s)
- Graph expansion (~1-3s)
- Claude API call with large context (~20-90s)

---

## Issues Identified

### Critical Issues

1. **Anthropic API Rate Limiting (429 Errors)**
   - Organization limit: 30,000 input tokens/minute
   - Caused 3 of 5 questions to fail in latest run
   - Error: `rate_limit_error` after large context submissions

   **Resolution:**
   - Add retry with exponential backoff
   - Reduce context window size
   - Request rate limit increase

2. **LLM Judge Not Scoring**
   - All `judgment` fields are `null`
   - Pass rate shows 0% despite good answers
   - Likely cause: OpenAI API key issue or judge initialization

   **Resolution:**
   - Verify `OPENAI_API_KEY` environment variable
   - Add error logging to LLMJudge class
   - Test judge independently

3. **Validation Model Not Found**
   - Model `claude-haiku-3-5-20241022` returns 404
   - Answer validation step failing silently

   **Resolution:**
   - Update to valid model: `claude-3-haiku-20240307`

### Minor Issues

4. **Neo4j Authentication**
   - Different container running on ports 7474/7687
   - Graph expansion disabled but doesn't block evaluation

5. **High Latency**
   - 35-100s per question is too slow for interactive use
   - Acceptable for batch evaluation

---

## Recommendations

### Immediate (Before Next Eval Run)

1. **Fix Rate Limiting**
   ```python
   # Add to src/agent/nodes.py
   @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=60))
   async def generate_answer(state):
       ...
   ```

2. **Fix Validation Model**
   ```python
   # Update config/settings.py
   validation_model: str = "claude-3-haiku-20240307"
   ```

3. **Verify OpenAI Key for Judge**
   ```bash
   export OPENAI_API_KEY="sk-..."
   ```

### Short-Term (Next Sprint)

4. **Reduce Context Size**
   - Limit retrieved chunks to top 5 instead of 10
   - Summarize long statute sections before inclusion

5. **Add Caching**
   - Cache Weaviate results for repeated queries
   - Cache graph expansions

### Long-Term

6. **Parallel Evaluation**
   - Run questions concurrently (with rate limit awareness)
   - Could reduce total eval time from ~12 min to ~3 min

---

## Baseline Established

This evaluation establishes the **v1.0 baseline** for the Florida Tax RAG system:

| Metric | Baseline | Target | Gap |
|--------|----------|--------|-----|
| Citation Precision | 85% | 90% | 5% |
| Citation Recall | 50% | 80% | 30% |
| Citation F1 | 58% | 85% | 27% |
| Pass Rate | TBD | 80% | - |
| Avg Latency | 50s | 10s | 40s |
| Hallucinations | 0 | 0 | Met |

### Priority Improvements

1. **Improve Recall** - Ensure all relevant statutes are retrieved
2. **Reduce Latency** - Context optimization and caching
3. **Enable Judge** - Get accurate quality scores

---

## Next Steps

1. Fix the three critical issues identified above
2. Run full 20-question evaluation with working judge
3. Analyze by difficulty level (easy/medium/hard)
4. Identify specific questions causing failures
5. Iterate on retrieval strategy based on recall gaps

---

## Appendix: Sample Answers

### eval_001: "What is the Florida state sales tax rate?"

**Expected:** 6% rate, cite 212.05

**Generated (excerpt):**
> The Florida state sales tax rate is **6 percent**.
>
> The fundamental Florida state sales tax rate is established in Florida Statute ยง 212.05(1)(a)1.a., which states:
> "At the rate of 6 percent of the sales price of each item or article of tangible personal property when sold at retail in this state..."
>
> Exceptions:
> - Electrical power or energy: 4.35%
> - Coin-operated amusement machines: 4%
> - New mobile homes: 3%

**Assessment:** Excellent answer. Correctly cites statute, provides rate, includes exceptions.

### eval_002: "Are groceries exempt from Florida sales tax?"

**Expected:** Yes (general food exempt), cite 212.08

**Generated (excerpt):**
> Yes, food products for human consumption (groceries) are generally exempt from Florida sales tax, but with significant exceptions for prepared foods and certain items.
>
> Food products for human consumption are exempt from the tax imposed by this chapter. [Source: ยง 212.08(1)(a)]
>
> Exceptions (NOT exempt):
> - Food sold as meals for consumption on or off premises
> - Ready-to-eat sandwiches
> - Hot prepared food products
> - Soft drinks, candy, ice cream

**Assessment:** Excellent answer. Correctly explains exemption with nuanced exceptions.
